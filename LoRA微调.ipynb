{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cde26cd3-6596-4374-8472-fc7e820477fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.integrations import TensorBoardCallback\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, HfArgumentParser\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from dataclasses import dataclass, field\n",
    "import datasets\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460425e-72d3-4732-bb47-425a79bf4f8e",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09015ad0-2e3b-4d69-a209-ed23ae7aa93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ba18cd37ad46fcbc8be9f0b6206f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/root/autodl-tmp/model/\"\n",
    "model_name = \"chatglm\"\n",
    "model_path = os.path.join(model_dir,model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, load_in_8bit=False, trust_remote_code=True, device_map=\"auto\")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1e0c31-9e26-43c0-b52e-66e7218f24f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (embedding): Embedding(\n",
      "      (word_embeddings): Embedding(65024, 4096)\n",
      "    )\n",
      "    (rotary_pos_emb): RotaryEmbedding()\n",
      "    (encoder): GLMTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x GLMBlock(\n",
      "          (input_layernorm): RMSNorm()\n",
      "          (self_attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
      "            (core_attention): CoreAttention(\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (post_attention_layernorm): RMSNorm()\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
      "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): RMSNorm()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880dda5f-167c-404a-93ee-d1869926be7f",
   "metadata": {},
   "source": [
    "# æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e80b26d-1f43-42eb-823e-6605f8f636b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "@dataclass\n",
    "class FinetuneArguments:\n",
    "    dataset_path: str = field(default=\"data/alpaca\")\n",
    "    model_path: str = field(default=\"output\")\n",
    "    lora_rank: int = field(default=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2a9d96a-9930-4aa3-99d8-8c9f0c904be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/model/chatglm-lora\",          # è¾“å‡ºç›®å½•\n",
    "    evaluation_strategy=\"epoch\",     # è¯„ä¼°ç­–ç•¥\n",
    "    per_device_train_batch_size=16,   # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹é‡å¤§å°\n",
    "    per_device_eval_batch_size=16,    # æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹é‡å¤§å°\n",
    "    num_train_epochs=3,              # è®­ç»ƒçš„æ€»è½®æ•°\n",
    "    learning_rate=2e-5,              # å­¦ä¹ ç‡\n",
    "    logging_dir=\"./logs\",            # æ—¥å¿—ä¿å­˜ç›®å½•\n",
    "    logging_steps=100,                # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "    save_steps=500,                  # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    save_total_limit=2,              # ä»…ä¿ç•™æœ€è¿‘çš„ä¸¤ä¸ªæ£€æŸ¥ç‚¹\n",
    ")\n",
    "finetune_args = FinetuneArguments(\n",
    "    dataset_path =\"/root/autodl-tmp/data/wenlv/wenlv_token\",\n",
    "    model_path =\"/root/autodl-tmp/model/chatglm-lora\",\n",
    "    lora_rank = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98081552-3722-43cc-8dfb-7bb3fe2b3824",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinetuneArguments(dataset_path='/root/autodl-tmp/data/wenlv/wenlv_token', model_path='/root/autodl-tmp/model/chatglm-lora', lora_rank=16) TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/root/autodl-tmp/model/chatglm-lora,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/autodl-tmp/model/chatglm-lora,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "print(finetune_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77890458-a917-4269-ad71-83ce4cece297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ModifiedTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         return model(input_ids=inputs[\"input_ids\"],labels=inputs[\"labels\"],).loss\n",
    "#     def save_model(self, output_dir=None, _internal_call=False):\n",
    "#         from transformers.trainer import TRAINING_ARGS_NAME\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "#         saved_params = {\n",
    "#             k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
    "#         }\n",
    "#         torch.save(saved_params, os.path.join(output_dir, \"lora_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "030fbd55-789a-4151-b8bb-dc49d8d2bb72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,899,392 || all params: 6,247,483,392 || trainable%: 0.0624\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=finetune_args.lora_rank,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    ")\n",
    "#modelæ‰“ä¸Šäº†loraçš„å¤–æŒ‚äº†ï¼Œä¼šæŠŠåŸæœ‰å‚æ•°éƒ½å†»ç»“ä½\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "772c69b2-9e51-4df2-b100-ee4f02315da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(train_dataset)=2956\n",
      "\n",
      "{'input_ids': [64790, 64792, 29101, 30954, 30910, 51074, 55177, 34953, 30970, 30938, 33123, 13, 4244, 1902, 266, 30954, 30910, 48576, 57504, 36841, 37495, 54638, 2], 'seq_len': 17}\n",
      "Instruction: æ™¯å¾·é•‡æœ‰å“ªäº›5Aæ™¯åŒº\n",
      "Answer:  å¤çª‘æ°‘ä¿—åšè§ˆåŒº\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.load_from_disk(finetune_args.dataset_path)\n",
    "print(f\"\\n{len(train_dataset)=}\\n\")\n",
    "index=1650\n",
    "example = train_dataset[1000]\n",
    "print(example)\n",
    "print(tokenizer.decode(example['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "794ac0e6-9305-4c49-a62a-2501822228d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/555 17:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.735700</td>\n",
       "      <td>2.103516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.080900</td>\n",
       "      <td>2.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.083600</td>\n",
       "      <td>2.015625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collate_data(features):\n",
    "    # print(features)\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    index=0\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        seq_len = feature['input_ids'].index(13)+1\n",
    "        # print(feature['input_ids'])\n",
    "        # print(feature['input_ids'][0:seq_len])\n",
    "        # print(tokenizer.decode(feature['input_ids'][0:seq_len], skip_special_tokens=True))\n",
    "        ids = feature[\"input_ids\"]\n",
    "        labels = ([-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l))\n",
    "        #print(labels)\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        # print(\"input_ids\",tokenizer.decode(input_ids[index].detach().tolist(), skip_special_tokens=True))\n",
    "        # print(\"labels_list\",tokenizer.decode(labels_list[index].detach().tolist(), skip_special_tokens=True))\n",
    "        index+=1\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return { \"input_ids\": input_ids,\"labels\": labels,}\n",
    "\n",
    "# trainer = ModifiedTrainer(model=model,train_dataset=train_dataset,eval_dataset=train_dataset,args=training_args,data_collator=collate_data,)#\n",
    "trainer = Trainer(model=model,train_dataset=train_dataset,eval_dataset=train_dataset,args=training_args,data_collator=collate_data,)#\n",
    "trainer.train()\n",
    "writer.close()\n",
    "# save model\n",
    "model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32930430-ffee-40c0-b6be-7ccf1c555d13",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72123fd4-9308-42bb-b059-cc9d8f973df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "from peft import PrefixEncoder, PrefixTuningConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747109f6-0d51-469b-823a-659f36b938d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#åˆ†è¯å™¨ ä»ç„¶ç”¨åŸç”Ÿçš„\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/model/chatglm\", trust_remote_code=True)\n",
    "#åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æ¨¡å‹\n",
    "model = AutoModel.from_pretrained(\"/root/autodl-tmp/model/chatglm\", trust_remote_code=True).cuda()\n",
    "model_lora = PeftModel.from_pretrained(model, \"/root/autodl-tmp/model/chatglm-lora\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2309f023-f66e-41fa-a67d-ca549c7c23e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ±Ÿè¥¿å©ºæºæ˜¯ä¸€å¤„ä»¥è‡ªç„¶é£å…‰å’Œäººæ–‡æ™¯è§‚è‘—ç§°çš„æ—…æ¸¸èƒœåœ°,ä»¥ä¸‹æ˜¯ä¸€äº›æ¨èçš„æ—…æ¸¸è·¯çº¿:\n",
      "\n",
      "ç¬¬ä¸€å¤©:\n",
      "1. æ—©ä¸Š:ä»å—æ˜Œå‡ºå‘,ä¹˜åç«è½¦æˆ–é•¿é€”æ±½è½¦å‰å¾€å©ºæºã€‚\n",
      "2. ä¸Šåˆ:æ¸¸è§ˆå©ºæºæœ€è‘—åçš„æ™¯ç‚¹â€”â€”æ²³å²­æ‘ã€‚è¯¥æ‘åº„ä»¥ç‹¬ç‰¹çš„æ°‘ä¿—æ–‡åŒ–å’Œå»ºç­‘é£æ ¼è€Œé—»å,æ˜¯ä¸€ä¸ªé€‚åˆæ¼«æ­¥å’Œæ‹ç…§çš„åœ°æ–¹ã€‚\n",
      "3. ä¸­åˆ:åœ¨æ²³å²­æ‘é™„è¿‘çš„é¤å…äº«ç”¨å½“åœ°ç¾é£Ÿã€‚\n",
      "4. ä¸‹åˆ:å‚è§‚æ±Ÿæ¹¾æ™¯åŒºã€‚è¿™é‡Œæœ‰ç¾ä¸½çš„æ¹–æ³Šå’Œå±±å³¦,å¯ä»¥æ¬£èµåˆ°å£®ä¸½çš„æ™¯è‰²ã€‚\n",
      "5. æ™šä¸Š:è¿”å›å—æ˜Œã€‚\n",
      "\n",
      "ç¬¬äºŒå¤©:\n",
      "1. æ—©ä¸Š:åœ¨å©ºæºå¿åŸäº«ç”¨æ—©é¤ã€‚\n",
      "2. ä¸Šåˆ:æ¸¸è§ˆç´«æ¸…å±±ã€‚è¿™æ˜¯ä¸€åº§æµ·æ‹”è¾ƒé«˜çš„å±±,å¯ä»¥æ¬£èµåˆ°å£®ä¸½çš„å±±å³°å’Œäº‘æµ·ã€‚\n",
      "3. ä¸­åˆ:åœ¨ç´«æ¸…å±±çš„é¤é¦†äº«ç”¨åˆé¤ã€‚\n",
      "4. ä¸‹åˆ:å‚è§‚æ±Ÿæ¹¾æ™¯åŒºã€‚åœ¨è¿™é‡Œ,å¯ä»¥æ¬£èµåˆ°ç¾ä¸½çš„æ¹–æ³Šå’Œå±±å³¦,å¯ä»¥åœ¨è¿™é‡Œæ¼«æ­¥æˆ–éª‘è¡Œã€‚\n",
      "5. æ™šä¸Š:åœ¨å©ºæºå¿åŸäº«ç”¨æ™šé¤ã€‚\n",
      "\n",
      "ç¬¬ä¸‰å¤©:\n",
      "1. æ—©ä¸Š:åœ¨å©ºæºå¿åŸäº«ç”¨æ—©é¤ã€‚\n",
      "2. ä¸Šåˆ:å‰å¾€çŸ³åŸå±±ã€‚è¿™æ˜¯ä¸€åº§æµ·æ‹”è¾ƒé«˜çš„å±±,å¯ä»¥æ¬£èµåˆ°å£®ä¸½çš„å±±å³°å’Œäº‘æµ·ã€‚\n",
      "3. ä¸­åˆ:åœ¨çŸ³åŸå±±çš„é¤é¦†äº«ç”¨åˆé¤ã€‚\n",
      "4. ä¸‹åˆ:æ¸¸è§ˆæ¸…åå›­ã€‚è¿™æ˜¯ä¸€ä¸ªç¾ä¸½çš„å›­æ—,æœ‰ç€ç‹¬ç‰¹çš„å»ºç­‘é£æ ¼å’Œæ–‡åŒ–åº•è•´ã€‚\n",
      "5. æ™šä¸Š:åœ¨å©ºæºå¿åŸäº«ç”¨æ™šé¤ã€‚\n",
      "\n",
      "ç¬¬å››å¤©:\n",
      "1. æ—©ä¸Š:åœ¨å©ºæºå¿åŸäº«ç”¨æ—©é¤ã€‚\n",
      "2. ä¸Šåˆ:å‰å¾€ä¸‰æ¸…å±±ã€‚è¿™æ˜¯ä¸€åº§æµ·æ‹”è¾ƒé«˜çš„å±±,æœ‰ç€ç‹¬ç‰¹çš„è‡ªç„¶é£å…‰å’Œæ–‡åŒ–åº•è•´ã€‚\n",
      "3. ä¸­åˆ:åœ¨ä¸‰æ¸…å±±çš„é¤é¦†äº«ç”¨åˆé¤ã€‚\n",
      "4. ä¸‹åˆ:æ¸¸è§ˆæ²³å²­æ‘ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰ç€ç‹¬ç‰¹æ°‘ä¿—æ–‡åŒ–å’Œå»ºç­‘é£æ ¼çš„æ‘åº„,éå¸¸é€‚åˆæ¼«æ­¥å’Œæ‹ç…§ã€‚\n",
      "5. æ™šä¸Š:è¿”å›å—æ˜Œã€‚\n",
      "\n",
      "ä»¥ä¸Šæ˜¯ä¸€äº›æ¨èçš„æ±Ÿè¥¿å©ºæºæ—…æ¸¸è·¯çº¿,å½“ç„¶è¿˜æœ‰å…¶ä»–å¾ˆå¤šå€¼å¾—æ¸¸è§ˆçš„æ™¯ç‚¹,å¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´è¿›è¡Œå®‰æ’ã€‚\n",
      "______________________________________________\n",
      "æ±Ÿè¥¿å©ºæºæ˜¯ä¸€å¤„ä»¥è‡ªç„¶é£å…‰å’Œäººæ–‡æ™¯è§‚è‘—ç§°çš„æ—…æ¸¸èƒœåœ°,ä¸‹é¢æ˜¯ä¸€ä»½è¾ƒä¸ºè¯¦ç»†çš„å©ºæºæ—…æ¸¸è·¯çº¿:\n",
      "\n",
      "ç¬¬ä¸€å¤©:\n",
      "1. ä»å—æ˜Œå‡ºå‘,ä¹˜åç«è½¦æˆ–é•¿é€”æ±½è½¦å‰å¾€å©ºæºã€‚\n",
      "2. æŠµè¾¾å©ºæºå,å‰å¾€æ±Ÿæ¹¾æ™¯åŒº,æ¸¸è§ˆé•¿çº¦3å…¬é‡Œçš„å°æ¡¥æµæ°´ã€ç²‰å¢™é»›ç“¦çš„å¾½æ´¾å»ºç­‘ç¾¤è½,æ¬£èµç¾ä¸½çš„æ°´ä¹¡é£å…‰ã€‚\n",
      "3. ä¸­åˆåœ¨æ™¯åŒºå†…çš„é¤å…å“å°å½“åœ°ç‰¹è‰²ç¾é£Ÿ,å¦‚æ¿æ —ã€ç³Šè±†è…ã€ç³¯ç±³é¥­å›¢ç­‰ã€‚\n",
      "4. ä¸‹åˆå‰å¾€æ±Ÿæ¹¾å¤é•‡,æ¸¸è§ˆä¿å­˜å®Œå¥½çš„æ˜æ¸…å»ºç­‘ç¾¤,ä½“éªŒå¤é•‡çš„é£æƒ…å¤éŸµ,å“å°å½“åœ°å°åƒã€‚\n",
      "5. æ™šä¸Šè¿”å›é…’åº—ä¼‘æ¯ã€‚\n",
      "\n",
      "ç¬¬äºŒå¤©:\n",
      "1. æ—©é¤åå‰å¾€ç¯å²­æ™¯åŒº,æ¸¸è§ˆæ‹¥æœ‰ç‹¬ç‰¹åœ°å½¢çš„ç´«å¾®å±±,ç™»é¡¶è¿œçœº,ä¿¯ç°æ•´ä¸ªæ™¯åŒºçš„ç¾æ™¯ã€‚\n",
      "2. ä¸­åˆåœ¨æ™¯åŒºå†…çš„é¤å…å“å°å½“åœ°ç‰¹è‰²ç¾é£Ÿ,å¦‚é¦™è‡ç‚–é¸¡ã€ç²‰è’¸è‚‰ã€èŒ¶å¶è›‹ç­‰ã€‚\n",
      "3. ä¸‹åˆå‰å¾€æ™¯åŒºå†…çš„æ±Ÿæ¹¾å¤é•‡,æ¸¸è§ˆä¿å­˜å®Œå¥½çš„æ˜æ¸…å»ºç­‘ç¾¤,ä½“éªŒå¤é•‡çš„é£æƒ…å¤éŸµ,å“å°å½“åœ°å°åƒã€‚\n",
      "4. æ™šä¸Šè¿”å›é…’åº—ä¼‘æ¯ã€‚\n",
      "\n",
      "ç¬¬ä¸‰å¤©:\n",
      "1. æ—©é¤åå‰å¾€æ€æºªæ™¯åŒº,æ¸¸è§ˆæ‹¥æœ‰ç‹¬ç‰¹åœ°å½¢çš„ç‰›è‚å²©,ç™»é¡¶è¿œçœº,ä¿¯ç°æ•´ä¸ªæ™¯åŒºçš„ç¾æ™¯ã€‚\n",
      "2. ä¸­åˆåœ¨æ™¯åŒºå†…çš„é¤å…å“å°å½“åœ°ç‰¹è‰²ç¾é£Ÿ,å¦‚ç‚’æ²³ç²‰ã€ç³¯ç±³é¸¡ã€èŒ¶å¶è›‹ç­‰ã€‚\n",
      "3. ä¸‹åˆå‰å¾€æ™¯åŒºå†…çš„æ±Ÿæ¹¾å¤é•‡,æ¸¸è§ˆä¿å­˜å®Œå¥½çš„æ˜æ¸…å»ºç­‘ç¾¤,ä½“éªŒå¤é•‡çš„é£æƒ…å¤éŸµ,å“å°å½“åœ°å°åƒã€‚\n",
      "4. æ™šä¸Šè¿”å›é…’åº—ä¼‘æ¯ã€‚\n",
      "\n",
      "ç¬¬å››å¤©:\n",
      "1. æ—©é¤åå‰å¾€ç†å‘æ™¯åŒº,æ¸¸è§ˆæ‹¥æœ‰ç‹¬ç‰¹åœ°å½¢çš„è€Œä¸‹é¾™æ½­,ç™»é¡¶è¿œçœº,ä¿¯ç°æ•´ä¸ªæ™¯åŒºçš„ç¾æ™¯ã€‚\n",
      "2. ä¸­åˆåœ¨æ™¯åŒºå†…çš„é¤å…å“å°å½“åœ°ç‰¹è‰²ç¾é£Ÿ,å¦‚ç‚–æ’éª¨ã€ç²‰è’¸ç±³ç²‰ã€æ²¹çˆ†è™¾ç­‰ã€‚\n",
      "3. ä¸‹åˆå‰å¾€æ™¯åŒºå†…çš„æ±Ÿæ¹¾å¤é•‡,æ¸¸è§ˆä¿å­˜å®Œå¥½çš„æ˜æ¸…å»ºç­‘ç¾¤,ä½“éªŒå¤é•‡çš„é£æƒ…å¤éŸµ,å“å°å½“åœ°å°åƒã€‚\n",
      "4. æ™šä¸Šè¿”å›é…’åº—ä¼‘æ¯ã€‚\n",
      "\n",
      "ç¬¬äº”å¤©:\n",
      "1. æ—©é¤åæ•´ç†è¡Œæé€€æˆ¿ã€‚\n",
      "2. å‰å¾€å©ºæºç«è½¦ç«™æˆ–æ±½è½¦ç«™,ä¹˜åç«è½¦æˆ–é•¿é€”æ±½è½¦è¿”å›å—æ˜Œã€‚\n",
      "\n",
      "è¿™ä»½å©ºæºæ—…æ¸¸è·¯çº¿ä»…ä¾›å‚è€ƒ,å…·ä½“è¡Œç¨‹å¯ä»¥æ ¹æ®ä¸ªäººéœ€è¦å’Œæ—¶é—´æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚\n"
     ]
    }
   ],
   "source": [
    "input=\"æ±Ÿè¥¿å©ºæºæ”»ç•¥æ—…æ¸¸è·¯çº¿\"#\"ç»„ç»‡å¤§å­¦éƒ¨é—¨å…­ä¸ƒä¸ªäººå»æ­¦åŠŸå±±æ—…æ¸¸ï¼Œæ±‚æ”»ç•¥ï¼Œè·¯çº¿\"\n",
    "response, history = model.chat(tokenizer, input, history=[])\n",
    "print(response)\n",
    "print(\"______________________________________________\")\n",
    "response, history = model_lora.chat(tokenizer, input, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a422dc-00a6-4a1d-83a8-e5cc7104053e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
