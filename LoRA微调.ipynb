{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cde26cd3-6596-4374-8472-fc7e820477fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.integrations import TensorBoardCallback\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, HfArgumentParser\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from dataclasses import dataclass, field\n",
    "import datasets\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460425e-72d3-4732-bb47-425a79bf4f8e",
   "metadata": {},
   "source": [
    "# 模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09015ad0-2e3b-4d69-a209-ed23ae7aa93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ba18cd37ad46fcbc8be9f0b6206f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/root/autodl-tmp/model/\"\n",
    "model_name = \"chatglm\"\n",
    "model_path = os.path.join(model_dir,model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, load_in_8bit=False, trust_remote_code=True, device_map=\"auto\")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1e0c31-9e26-43c0-b52e-66e7218f24f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (embedding): Embedding(\n",
      "      (word_embeddings): Embedding(65024, 4096)\n",
      "    )\n",
      "    (rotary_pos_emb): RotaryEmbedding()\n",
      "    (encoder): GLMTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x GLMBlock(\n",
      "          (input_layernorm): RMSNorm()\n",
      "          (self_attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
      "            (core_attention): CoreAttention(\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (post_attention_layernorm): RMSNorm()\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
      "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): RMSNorm()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880dda5f-167c-404a-93ee-d1869926be7f",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e80b26d-1f43-42eb-823e-6605f8f636b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "@dataclass\n",
    "class FinetuneArguments:\n",
    "    dataset_path: str = field(default=\"data/alpaca\")\n",
    "    model_path: str = field(default=\"output\")\n",
    "    lora_rank: int = field(default=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2a9d96a-9930-4aa3-99d8-8c9f0c904be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/model/chatglm-lora\",          # 输出目录\n",
    "    evaluation_strategy=\"epoch\",     # 评估策略\n",
    "    per_device_train_batch_size=16,   # 每个设备的训练批量大小\n",
    "    per_device_eval_batch_size=16,    # 每个设备的评估批量大小\n",
    "    num_train_epochs=3,              # 训练的总轮数\n",
    "    learning_rate=2e-5,              # 学习率\n",
    "    logging_dir=\"./logs\",            # 日志保存目录\n",
    "    logging_steps=100,                # 每10步记录一次日志\n",
    "    save_steps=500,                  # 每500步保存一次模型\n",
    "    save_total_limit=2,              # 仅保留最近的两个检查点\n",
    ")\n",
    "finetune_args = FinetuneArguments(\n",
    "    dataset_path =\"/root/autodl-tmp/data/wenlv/wenlv_token\",\n",
    "    model_path =\"/root/autodl-tmp/model/chatglm-lora\",\n",
    "    lora_rank = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98081552-3722-43cc-8dfb-7bb3fe2b3824",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinetuneArguments(dataset_path='/root/autodl-tmp/data/wenlv/wenlv_token', model_path='/root/autodl-tmp/model/chatglm-lora', lora_rank=16) TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/root/autodl-tmp/model/chatglm-lora,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/autodl-tmp/model/chatglm-lora,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "print(finetune_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77890458-a917-4269-ad71-83ce4cece297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ModifiedTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         return model(input_ids=inputs[\"input_ids\"],labels=inputs[\"labels\"],).loss\n",
    "#     def save_model(self, output_dir=None, _internal_call=False):\n",
    "#         from transformers.trainer import TRAINING_ARGS_NAME\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "#         saved_params = {\n",
    "#             k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
    "#         }\n",
    "#         torch.save(saved_params, os.path.join(output_dir, \"lora_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "030fbd55-789a-4151-b8bb-dc49d8d2bb72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,899,392 || all params: 6,247,483,392 || trainable%: 0.0624\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=finetune_args.lora_rank,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    ")\n",
    "#model打上了lora的外挂了，会把原有参数都冻结住\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "772c69b2-9e51-4df2-b100-ee4f02315da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(train_dataset)=2956\n",
      "\n",
      "{'input_ids': [64790, 64792, 29101, 30954, 30910, 51074, 55177, 34953, 30970, 30938, 33123, 13, 4244, 1902, 266, 30954, 30910, 48576, 57504, 36841, 37495, 54638, 2], 'seq_len': 17}\n",
      "Instruction: 景德镇有哪些5A景区\n",
      "Answer:  古窑民俗博览区\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.load_from_disk(finetune_args.dataset_path)\n",
    "print(f\"\\n{len(train_dataset)=}\\n\")\n",
    "index=1650\n",
    "example = train_dataset[1000]\n",
    "print(example)\n",
    "print(tokenizer.decode(example['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "794ac0e6-9305-4c49-a62a-2501822228d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/555 17:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.735700</td>\n",
       "      <td>2.103516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.080900</td>\n",
       "      <td>2.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.083600</td>\n",
       "      <td>2.015625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collate_data(features):\n",
    "    # print(features)\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    index=0\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        seq_len = feature['input_ids'].index(13)+1\n",
    "        # print(feature['input_ids'])\n",
    "        # print(feature['input_ids'][0:seq_len])\n",
    "        # print(tokenizer.decode(feature['input_ids'][0:seq_len], skip_special_tokens=True))\n",
    "        ids = feature[\"input_ids\"]\n",
    "        labels = ([-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l))\n",
    "        #print(labels)\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        # print(\"input_ids\",tokenizer.decode(input_ids[index].detach().tolist(), skip_special_tokens=True))\n",
    "        # print(\"labels_list\",tokenizer.decode(labels_list[index].detach().tolist(), skip_special_tokens=True))\n",
    "        index+=1\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return { \"input_ids\": input_ids,\"labels\": labels,}\n",
    "\n",
    "# trainer = ModifiedTrainer(model=model,train_dataset=train_dataset,eval_dataset=train_dataset,args=training_args,data_collator=collate_data,)#\n",
    "trainer = Trainer(model=model,train_dataset=train_dataset,eval_dataset=train_dataset,args=training_args,data_collator=collate_data,)#\n",
    "trainer.train()\n",
    "writer.close()\n",
    "# save model\n",
    "model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32930430-ffee-40c0-b6be-7ccf1c555d13",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72123fd4-9308-42bb-b059-cc9d8f973df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "from peft import PrefixEncoder, PrefixTuningConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747109f6-0d51-469b-823a-659f36b938d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#分词器 仍然用原生的\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/model/chatglm\", trust_remote_code=True)\n",
    "#加载训练好的模型模型\n",
    "model = AutoModel.from_pretrained(\"/root/autodl-tmp/model/chatglm\", trust_remote_code=True).cuda()\n",
    "model_lora = PeftModel.from_pretrained(model, \"/root/autodl-tmp/model/chatglm-lora\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2309f023-f66e-41fa-a67d-ca549c7c23e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江西婺源是一处以自然风光和人文景观著称的旅游胜地,以下是一些推荐的旅游路线:\n",
      "\n",
      "第一天:\n",
      "1. 早上:从南昌出发,乘坐火车或长途汽车前往婺源。\n",
      "2. 上午:游览婺源最著名的景点——河岭村。该村庄以独特的民俗文化和建筑风格而闻名,是一个适合漫步和拍照的地方。\n",
      "3. 中午:在河岭村附近的餐厅享用当地美食。\n",
      "4. 下午:参观江湾景区。这里有美丽的湖泊和山峦,可以欣赏到壮丽的景色。\n",
      "5. 晚上:返回南昌。\n",
      "\n",
      "第二天:\n",
      "1. 早上:在婺源县城享用早餐。\n",
      "2. 上午:游览紫清山。这是一座海拔较高的山,可以欣赏到壮丽的山峰和云海。\n",
      "3. 中午:在紫清山的餐馆享用午餐。\n",
      "4. 下午:参观江湾景区。在这里,可以欣赏到美丽的湖泊和山峦,可以在这里漫步或骑行。\n",
      "5. 晚上:在婺源县城享用晚餐。\n",
      "\n",
      "第三天:\n",
      "1. 早上:在婺源县城享用早餐。\n",
      "2. 上午:前往石城山。这是一座海拔较高的山,可以欣赏到壮丽的山峰和云海。\n",
      "3. 中午:在石城山的餐馆享用午餐。\n",
      "4. 下午:游览清华园。这是一个美丽的园林,有着独特的建筑风格和文化底蕴。\n",
      "5. 晚上:在婺源县城享用晚餐。\n",
      "\n",
      "第四天:\n",
      "1. 早上:在婺源县城享用早餐。\n",
      "2. 上午:前往三清山。这是一座海拔较高的山,有着独特的自然风光和文化底蕴。\n",
      "3. 中午:在三清山的餐馆享用午餐。\n",
      "4. 下午:游览河岭村。这是一个有着独特民俗文化和建筑风格的村庄,非常适合漫步和拍照。\n",
      "5. 晚上:返回南昌。\n",
      "\n",
      "以上是一些推荐的江西婺源旅游路线,当然还有其他很多值得游览的景点,可以根据自己的兴趣和时间进行安排。\n",
      "______________________________________________\n",
      "江西婺源是一处以自然风光和人文景观著称的旅游胜地,下面是一份较为详细的婺源旅游路线:\n",
      "\n",
      "第一天:\n",
      "1. 从南昌出发,乘坐火车或长途汽车前往婺源。\n",
      "2. 抵达婺源后,前往江湾景区,游览长约3公里的小桥流水、粉墙黛瓦的徽派建筑群落,欣赏美丽的水乡风光。\n",
      "3. 中午在景区内的餐厅品尝当地特色美食,如板栗、糊豆腐、糯米饭团等。\n",
      "4. 下午前往江湾古镇,游览保存完好的明清建筑群,体验古镇的风情古韵,品尝当地小吃。\n",
      "5. 晚上返回酒店休息。\n",
      "\n",
      "第二天:\n",
      "1. 早餐后前往篁岭景区,游览拥有独特地形的紫微山,登顶远眺,俯瞰整个景区的美景。\n",
      "2. 中午在景区内的餐厅品尝当地特色美食,如香菇炖鸡、粉蒸肉、茶叶蛋等。\n",
      "3. 下午前往景区内的江湾古镇,游览保存完好的明清建筑群,体验古镇的风情古韵,品尝当地小吃。\n",
      "4. 晚上返回酒店休息。\n",
      "\n",
      "第三天:\n",
      "1. 早餐后前往思溪景区,游览拥有独特地形的牛肝岩,登顶远眺,俯瞰整个景区的美景。\n",
      "2. 中午在景区内的餐厅品尝当地特色美食,如炒河粉、糯米鸡、茶叶蛋等。\n",
      "3. 下午前往景区内的江湾古镇,游览保存完好的明清建筑群,体验古镇的风情古韵,品尝当地小吃。\n",
      "4. 晚上返回酒店休息。\n",
      "\n",
      "第四天:\n",
      "1. 早餐后前往理坑景区,游览拥有独特地形的而下龙潭,登顶远眺,俯瞰整个景区的美景。\n",
      "2. 中午在景区内的餐厅品尝当地特色美食,如炖排骨、粉蒸米粉、油爆虾等。\n",
      "3. 下午前往景区内的江湾古镇,游览保存完好的明清建筑群,体验古镇的风情古韵,品尝当地小吃。\n",
      "4. 晚上返回酒店休息。\n",
      "\n",
      "第五天:\n",
      "1. 早餐后整理行李退房。\n",
      "2. 前往婺源火车站或汽车站,乘坐火车或长途汽车返回南昌。\n",
      "\n",
      "这份婺源旅游路线仅供参考,具体行程可以根据个人需要和时间情况进行调整。\n"
     ]
    }
   ],
   "source": [
    "input=\"江西婺源攻略旅游路线\"#\"组织大学部门六七个人去武功山旅游，求攻略，路线\"\n",
    "response, history = model.chat(tokenizer, input, history=[])\n",
    "print(response)\n",
    "print(\"______________________________________________\")\n",
    "response, history = model_lora.chat(tokenizer, input, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a422dc-00a6-4a1d-83a8-e5cc7104053e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
