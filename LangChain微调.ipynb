{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d746aafc-3c05-4cdb-9d57-d2b63e167769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel,AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd20b2a-fab5-4345-87e5-1df02aa485a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model_name=\"/root/autodl-tmp/model/text2vec\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a849111d-79bd-4e31-a6eb-9406cb8f843b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='/root/autodl-tmp/model/text2vec', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fccf42a-c666-4e34-a0f0-6da610d69a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(vector):\n",
    "    ss=sum([s**2 for s in vector])\n",
    "    return [round(s/ss,5) for s in vector]\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    # print(\"token_embeddings\")\n",
    "    # print(token_embeddings)\n",
    "    # print(len(token_embeddings),token_embeddings.size())\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    # print(\"input_mask_expanded\")\n",
    "    # print(input_mask_expanded)\n",
    "    result= torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)#求平均\n",
    "    # print(torch.sum(token_embeddings * input_mask_expanded, 1))\n",
    "    # print(torch.clamp(input_mask_expanded.sum(1), min=1e-9))\n",
    "    # print(result.shape)\n",
    "    result=result[0].tolist()\n",
    "    # print(len(result))\n",
    "    result=normal(result)\n",
    "    return result\n",
    "def get_vector(sentence):\n",
    "    encoded_input = tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')\n",
    "    # print(\"encoded_input\")\n",
    "    # print(encoded_input)\n",
    "    # print(tokenizer.decode(encoded_input['input_ids'][0], skip_special_tokens=True))\n",
    "    model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84767689-0d19-4d28-bd2f-43d90f980f65",
   "metadata": {},
   "source": [
    "# 建立知识向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b619ea-0593-4c1d-92d9-e79b913d3ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2956\n"
     ]
    }
   ],
   "source": [
    "id_knowadge={}\n",
    "with open(\"/root/autodl-tmp/data/wenlv/wenlv.jsonl\",encoding=\"utf-8\") as f:\n",
    "    lines=[json.loads(s.strip()) for s in f.readlines()]\n",
    "id_vector=[]\n",
    "print(len(lines))\n",
    "for i,data in enumerate(lines):\n",
    "    #if(i%100==0): print(i)\n",
    "    query=data[\"context\"].replace(\"Instruction: \",\"\").replace(\"\\nAnswer: \",\"\")\n",
    "    target=data[\"target\"]\n",
    "    #使用bert模型把问题进行向量化\n",
    "    vector=get_vector(query)\n",
    "    # #每条知识，对应一个id\n",
    "    # #id和向量\n",
    "    id_vector.append(vector)\n",
    "    # #id和对应的知识\n",
    "    id_knowadge[i]={\"query\":query,\"target\":target}\n",
    "id_vector=np.array(id_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972fd41e-118a-4936-922d-2cac89fd04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(id_vector[0])\n",
    "# print(id_vector[0].dtype)\n",
    "id_vector = id_vector.astype(np.float32)\n",
    "# print(id_vector_32[0])\n",
    "# print(id_vector_32[0].dtype)\n",
    "with open(\"id_knowadge\",\"w\") as f:\n",
    "    json.dump(id_knowadge,f,ensure_ascii=False)\n",
    "#id和向量用faiss库来存储\n",
    "#faiss是向量数据库，支持向量的高性能检索\n",
    "index = faiss.IndexFlatL2(768) \n",
    "# print(index.is_trained)\n",
    "index.add(id_vector)    \n",
    "# print(index.ntotal)\n",
    "with open(\"id_vector\",\"wb\") as f:\n",
    "    pickle.dump(index,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37c1f4-57f6-49c9-ba10-a63fa7cf59e4",
   "metadata": {},
   "source": [
    "# LangChain推理比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d62787c-8767-424f-98c7-ba7badec602a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc04b74bda742fbb8bae1dbd073d17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "glm_tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/model/chatglm\", trust_remote_code=True)\n",
    "glm_model = AutoModel.from_pretrained(\"/root/autodl-tmp/model/chatglm\", trust_remote_code=True).float()\n",
    "glm_config = AutoConfig.from_pretrained( \"/root/autodl-tmp/model/chatglm\", trust_remote_code=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3531311-875a-42e1-aad6-6ca0fa33f587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(sentences,query):\n",
    "    return \"请根据以下事实回答问题{}。问题是：{}\".format(\"。\".join(sentences),query)\n",
    "with open(\"id_vector\",\"rb\") as f:\n",
    "    index=pickle.load(f)\n",
    "with open(\"id_knowadge\") as f:\n",
    "    id_know=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "683e6f01-05ee-405a-a703-1e0accb6a3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['《梦里老家》实景演出门票价格：238元', '上饶市婺源县紫阳镇梦里老家文旅小镇', '《梦里老家》实景演出位于上饶市婺源县紫阳镇梦里老家文旅小镇。', '3月4日-12月31日20:00-21:10(停止售票19:45,最晚入园19:55)', '《梦里老家》实景演出']\n"
     ]
    }
   ],
   "source": [
    "input=\"梦里老家实景演出票价\"\n",
    "#把问题变成向量\n",
    "vector = get_vector(input)\n",
    "vector = np.array([vector])\n",
    "vector = vector.astype(np.float32)\n",
    "#从faiss库里面找到最近的5个id  D距离，I id\n",
    "D, I = index.search(vector, 5)\n",
    "D=D[0]\n",
    "I=I[0]\n",
    "sentences=[]\n",
    "for d,i in zip(D,I):\n",
    "    #距离过滤\n",
    "    if d>0.02:\n",
    "        continue\n",
    "    #print (id_know[str(i)]['query'])\n",
    "    sentences.append(id_know[str(i)]['target'])\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca22c40-acf2-4de2-93a7-432cdceb0a04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梦里老家实景演出票价和演出时间\n",
      "我无法提供具体的梦里老家实景演出的票价和演出时间，因为这些信息可能会随着演出地点、演出时间和其他因素的变化而有所不同。建议您查询相关演出网站或联系售票机构以获取最新的票价和演出时间信息。\n",
      "________________________________\n",
      "请根据以下事实回答问题《梦里老家》实景演出门票价格：238元。上饶市婺源县紫阳镇梦里老家文旅小镇。《梦里老家》实景演出位于上饶市婺源县紫阳镇梦里老家文旅小镇。。3月4日-12月31日20:00-21:10(停止售票19:45,最晚入园19:55)。《梦里老家》实景演出。问题是：梦里老家实景演出票价和演出时间\n",
      "梦里老家实景演出的票价为238元，演出时间为3月4日至12月31日，演出时间为20:00至21:10，停止售票时间为19:45，最晚入园时间为19:55。\n"
     ]
    }
   ],
   "source": [
    "prompt=\"梦里老家实景演出票价和演出时间\"\n",
    "print (prompt)\n",
    "response, history = glm_model.chat(glm_tokenizer, prompt, history=[])\n",
    "print(response)\n",
    "print(\"________________________________\")\n",
    "prompt=get_prompt(sentences,prompt)\n",
    "print (prompt)\n",
    "response, history = glm_model.chat(glm_tokenizer, prompt, history=[])\n",
    "print(response)\n",
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba24004-bf48-4e61-97cf-b413c97c86fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
